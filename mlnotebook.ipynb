{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning with Python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2\n",
    "\n",
    "### Learning Activity 1: Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries all imported, ready to go\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extra plotting functionality \n",
    "import visplots \n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats.distributions import randint\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "print \"libraries all imported, ready to go\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 2: Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using throughout this workshop is an adapted version of the wine quality case study, available from the UCI Machine Learning repository at https://archive.ics.uci.edu/ml/datasets/Wine+Quality. The first thing you will need to do in order to work with the wine quality dataset is to read the contents from the provided `wine_quality.csv` data file using the `read_csv` command. You should also try to explore the first few rows of the imported wine DataFrame using the `head` function from the `pandas` package (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.075</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.9</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.069</td>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17</td>\n",
       "      <td>102</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.097</td>\n",
       "      <td>15</td>\n",
       "      <td>65</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17</td>\n",
       "      <td>102</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.80</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "5            7.4              0.66         0.00             1.8      0.075   \n",
       "6            7.9              0.60         0.06             1.6      0.069   \n",
       "7            7.5              0.50         0.36             6.1      0.071   \n",
       "8            6.7              0.58         0.08             1.8      0.097   \n",
       "9            7.5              0.50         0.36             6.1      0.071   \n",
       "\n",
       "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates quality  \n",
       "0                   11                    34   0.9978  3.51       0.56     low  \n",
       "1                   25                    67   0.9968  3.20       0.68     low  \n",
       "2                   15                    54   0.9970  3.26       0.65     low  \n",
       "3                   17                    60   0.9980  3.16       0.58     low  \n",
       "4                   11                    34   0.9978  3.51       0.56     low  \n",
       "5                   13                    40   0.9978  3.51       0.56     low  \n",
       "6                   15                    59   0.9964  3.30       0.46     low  \n",
       "7                   17                   102   0.9978  3.35       0.80     low  \n",
       "8                   15                    65   0.9959  3.28       0.54     low  \n",
       "9                   17                   102   0.9978  3.35       0.80     low  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data and explore the first few rows\n",
    "\n",
    "wineQ = pd.read_csv(\"data/wine_quality.csv\",sep=\",\")\n",
    "header = wineQ.columns.values\n",
    "wineQ.head(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the data into our classification models and sklearn, the imported wine quality DataFrame needs to be converted into a `numpy` array. For more information on numpy arrays, see http://scipy-lectures.github.io/intro/numpy/array_object.html. \n",
    "\n",
    "In addition, it is always a good practice to **always** check the dimensionality of the imported data using the `shape` command prior to constructing any classification model to check that you really have imported all the data and imported it in the correct way (e.g. one common mistake is to get the separator wrong and end up with only one column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.4 0.7 0.0 ..., 3.51 0.56 'low']\n",
      " [7.8 0.88 0.0 ..., 3.2 0.68 'low']\n",
      " [7.8 0.76 0.04 ..., 3.26 0.65 'low']\n",
      " ..., \n",
      " [7.9 0.41 0.37 ..., 3.17 0.54 'high']\n",
      " [7.9 0.44 0.37 ..., 3.16 0.54 'high']\n",
      " [7.1 0.44 0.37 ..., 3.07 0.43 'high']]\n",
      "(1487, 11)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array and check the dimensionality\n",
    "npWineQ = np.array(wineQ)\n",
    "print npWineQ\n",
    "print npWineQ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 3: Inspect your data by indexing and index slicing\n",
    "\n",
    "To select elements in an array, you specify their indices with square bracket notation. For a two-dimensional array, the first index indicates the row number and the second index indicates the column number. Try selecting the values of the first and second columns of the first sample in the npArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4\n"
     ]
    }
   ],
   "source": [
    "# Print the 1st row and 1st column of npArray\n",
    "\n",
    "print npWineQ[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "# Print the 1st row and 2nd column of npArray\n",
    "print npWineQ[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select ranges of elements, we use \"index slicing\". Index slicing is the technical name for the syntax A[lower:upper], where lower refers to the lower bound index that is included, and upper refers to the upper bound index that is not included. Try selecting the first three samples (rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.4 0.7 0.0 1.9 0.076 11.0 34.0 0.9978 3.51 0.56 'low']\n",
      " [7.8 0.88 0.0 2.6 0.098 25.0 67.0 0.9968 3.2 0.68 'low']\n",
      " [7.8 0.76 0.04 2.3 0.092 15.0 54.0 0.997 3.26 0.65 'low']]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 3 rows of npArray\n",
    "print npWineQ[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and also the first three samples (rows) of the last column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low' 'low' 'low']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 3 rows from the last column of npArray\n",
    "print npWineQ[:3,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 4: Split the data into input features, X, and outputs, y\n",
    "\n",
    "Subsequently, we need to split our initial dataset into the data matrix X (independent variable) and the associated class vector y (dependent or target variable). The input features, _X_,  are the variables that you use to predict the outcome. In this data set, there are ten input features stored in columns 1-10 (index 0-9, although the upper bound is not included so the range for indexing is 0:10), all of which have continuous values. The output label, _y_, holds the information of whether the wine has been rated as high or low quality, and is stored in the final (eleventh) column (index 10). To split the data, we need to assign the columns of the input features and the columns of the output labels to different arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.4 0.7 0.0 ..., 0.9978 3.51 0.56]\n",
      " [7.8 0.88 0.0 ..., 0.9968 3.2 0.68]\n",
      " [7.8 0.76 0.04 ..., 0.997 3.26 0.65]\n",
      " ..., \n",
      " [7.9 0.41 0.37 ..., 0.992 3.17 0.54]\n",
      " [7.9 0.44 0.37 ..., 0.992 3.16 0.54]\n",
      " [7.1 0.44 0.37 ..., 0.9896 3.07 0.43]]\n",
      "['low' 'low' 'low' ..., 'high' 'high' 'high']\n"
     ]
    }
   ],
   "source": [
    "# Split to input matrix X and class vector y\n",
    "X = npWineQ[:,0:10]\n",
    "y = npWineQ[:,10]\n",
    "print X\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try printing the size of the input matrix _X_ and class vector _y_ using the \"`shape`\" command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions: (1487, 10)\n",
      "Y dimensions: (1487,)\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of X and y\n",
    "print \"X dimensions:\" , X.shape\n",
    "print \"Y dimensions:\" ,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis (EDA) is the field dealing with the analysis of data sets as a means of summarising their main characteristics, often using visual methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 5: Plot y frequencies \n",
    "\n",
    "An important thing to understand before applying any classification algorithms is how the output labels are distributed. Are they evenly distributed? Imbalances in distribution of labels can often lead to poor classification results for the minority class even if the classification results for the majority class are very good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['high' 500]\n",
      " ['low' 987]]\n"
     ]
    }
   ],
   "source": [
    "# Print the y frequencies\n",
    "yFreq = scipy.stats.itemfreq(y)\n",
    "print yFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current dataset, the _y_ values are categorical (i.e. they can only take one of a discrete set of values) and have a non-numeric representation, \"high\" vs. \"low\". This can be problematic for scikit-learn and plotting functions in Python, since they assume numerical values, so we need to map the text categories to numerical representations using `LabelEncoder`  and the `fit_transform` function from the `preprocessing` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 0 0 0]\n",
      "[[  0 500]\n",
      " [  1 987]]\n"
     ]
    }
   ],
   "source": [
    "# Convert the categorical to numeric values, and print the y frequencies\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "yFreq = scipy.stats.itemfreq(y)\n",
    "print y\n",
    "print yFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the data in some way is a good way to get a feel for how the data is distributed. As a simple example, try plotting the frequencies of the class labels (held in _yFreq_), 1 and 0, and see how they are distributed using the function `bar()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEPCAYAAABV6CMBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a0a8d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the y frequencies in a barplot\n",
    "fig, ax = plt.subplots()\n",
    "classFreq = yFreq[0][1], yFreq[1][1]\n",
    "classes = 'High Quality', 'Low Quality'\n",
    "plt.bar(range(len(classes)),classFreq, align='center', color=('blue','red'))\n",
    "\n",
    "plt.xlabel(classes)\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 6: Scale the data\n",
    "\n",
    "It is usually advisable to scale your data prior to fitting a classification model to avoid attributes with\n",
    "greater numeric ranges dominating those with smaller numeric ranges. Boxplots are a powerful visual aid, commonly used\n",
    "in order to investigate the differences in ranges of the input features. For example, try and plot the features of the _raw_ matrix _X_ using the script for the boxplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a boxplot of the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of scaling but one common scaling mechanism is auto-scaling, where for each\n",
    "column, the values are centred around the mean and divided by their standard deviation. This scaling\n",
    "mechanism can be applied by calling the `scale()` function in scikit-learn’s `preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Auto-scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we re-run the previous plotting script, we can have a look at the outcome of the boxplot after scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a boxplot of the scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 7:  Plot pairs of input features X as scatter plots\n",
    "\n",
    "You can visualise the relationship between two variables (features) using a simple scatter plot. This step can give you a good first indication of the ML model model to apply and its complexity (linear vs. non-linear). At this stage, let’s plot the first two variables against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter plot of the first two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also relate associations between features to their y classifications by making the colour of\n",
    "the points dependent on the corresponding y value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an enhanced scatter plot of the first two features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 8: Bonus 1 - Try different combinations of f1 and f2 (in a grid if you can).\n",
    "\n",
    "Hint: you may want to use nested loops, and the functions `subplot()` and `tight_layout()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a grid plot of scatterplots using a combination of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 9: Bonus 2 -  Try plotting different combinations of three features (f1, f2, f3) in the same plot.\n",
    "\n",
    "Hint: you may want to use the `Axes3D` function from the `mpl_toolkits.mplot3d` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a 3D scatterplot using the first three features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 10: Bonus 3 -  Create a correlation matrix and plot a heatmap of correlations between the input features in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the different features (variables) in X are not completely independent from each other. For example,\n",
    "fixed acidity is related to volatile acidity. To quickly identify which features are related and to\n",
    "what extent, it is useful to see how they are correlated. You can do this by creating a correlation matrix\n",
    "from X using `corrcoef()` in the `numpy` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for linear relationships between features across all pairs of features, you can use a heatmap\n",
    "of correlations (directly from X), which is simply a matrix of subplots whose colours represent the\n",
    "sizes of the correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a heatmap of the correlation coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3\n",
    "\n",
    "### Learning Activity 11: Split the data into training and test sets\n",
    "\n",
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split the wine dataset into two disjoint sets: train and test (**Holdout method**) using the `train_test_split` function. <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XTrain and yTrain are the two arrays you use to train your model. XTest and yTest are the two arrays that you use to evaluate your model. By default, scikit-learn splits the data so that 25% of it is used for testing, but you can also specify the proportion of data you want to use for training and testing.\n",
    "\n",
    "<br/>You can check the sizes of the different training and test sets by using the `shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the dimensionality of the individual splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also investigate how the class labels are distributed within the *yTest* vector by using the `itemfreq` function as previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the frequency of classes in yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 129 random samples of class 0 (high quality) and 243 random samples of class 1 (low quality) are included in the yTest set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 12: Apply KNN classification algorithm with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build KNN models using scikit-learn, you will be using the `KNeighborsClassifier` object, which allows you to set the value of K using the `n_neighbors` parameter (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). The optimal choice for the value K is highly data-dependent: in general a larger K suppresses the effects of noise, but makes the classification boundaries less distinct. <br/>\n",
    "\n",
    "\n",
    "For every classification model built with scikit-learn, we will follow four main steps: 1) **Building** the classification model (using either default, pre-defined or optimised parameters), 2) **Training** the model with data, 3) **Testing** the model, and 4) **Performance evaluation** using various metrics. <br/> <br/>\n",
    "\n",
    "We are going to start by trying two pre-defined random values of K and compare their performance. Let us start with a small number of K such as K=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a KNN classifier with 3 nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try a larger value of K, for instance K = 99 or another number of your own choice; remember, it is good practice to select an **odd** number for K in a binary classification problem to avoid ties. Can you generate the KNN model and print the overall performance for a larger K (such as K=99) using as guidance the previous example? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a KNN classifier with 99 nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 13: Calculate validation metrics for your classifier\n",
    "\n",
    "In a classification task, once you have created your predictive model, you will need to evaluate it. Evaluation functions help you to do this by reporting the performance of the model through four main performance metrics: precision, recall and specificity for the different classes, and overall accuracy. To understand these metrics, it is useful to create a _confusion matrix_, which records all the true positive, true negative, false positive and false negative values.\n",
    "\n",
    "We can compute the confusion matrix for our classifier using the `confusion_matrix` function in the `metrics` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the confusion matrix for your classifier using metrics.confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because performance metrics are such an important step of model evaluation, scikit-learn offers a wrapper around these functions, `metrics.classification_report`, to facilitate their computation. It also offers the function `metrics.accuracy_score` that we tried before to compute the overall accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Report the metrics using metrics.classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Learning Activity 14: Plot the decision boundaries for different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the KNN classifier using the built-in function `visplots.knnDecisionPlot`. For easier visualisation, only the test samples are depicted in the plot. Remember though that the decision boundary has been built using the _training_ data! <br/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the arguments of the function\n",
    "\n",
    "# Visualise the boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer: <BR/> For smaller values of K the decision boundaries present many \"creases\". In this case the models may suffer from instances of overfitting. For larger values of K, we can see that the decision boundaries are less distinct and tend towards linearity. In these cases the boundaries may be too simple and unable to learn thus leading to cases of underfitting. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 15 - Bonus: Try different weight configurations\n",
    "\n",
    "Under some circumstances, it is better to give more importance (\"weight\" in computing terms) to nearer neighbors. This can be accomplished through the `weights` parameter.  When `weights = 'distance'`, weights are assigned to the training data points in a way that is proportional to the inverse of the distance from the query point. In other words, nearer neighbors contribute more to the fit. <br/>\n",
    "\n",
    "What if we use weights based on distance? Does it improve the overall performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier with two pre-defined parameters (n_neighbors and weights)\n",
    "\n",
    "# Visualise the boundaries of a KNN model with weights equal to \"distance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4\n",
    "\n",
    "### Learning Activity 16: Implement k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us estimate the accuracy of the classifier on the wine quality dataset by splitting the data 5 consecutive times (the parameter cv gives the number of samples the data is split into) using the `cross_val_score` function. For example, try to implement cross-validation for knn3, your KNN model with k=3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement cross-validation for knn3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "\n",
    "### Learning Activity 17: Grid search on hyperparameters\n",
    "\n",
    "Rather than trying one-by-one predefined values of K, we can automate this process. The scikit-learn library provides the grid search function `GridSearchCV` (http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html), which allows us to exhaustively search for the optimum combination of parameters by evaluating models trained with a particular algorithm with all provided parameter combinations. Further details and examples on grid search with scikit-learn can be found at http://scikit-learn.org/stable/modules/grid_search.html <br/>\n",
    "\n",
    "You can use the `GridSearchCV` function with the validation technique of your choice (in this example, 10-fold cross-validation has been applied) to search for a parametisation of the KNN algorithm that gives a more optimal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Conduct a grid search with 10-fold cross-validation using the dictionary of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find and print the best parameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also graphically represent the results of the grid search using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the grid search results using a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the resulting model it is important to do it on held-out samples that were not seen during the grid search process (XTest). <Br/>\n",
    "So, we are testing our independent XTest dataset using the optimised model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by grid search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 18: Randomized search on hyperparameters\n",
    "\n",
    "Unlike `GridSearchCV`, `RandomizedSearchCV` does not exhaustively try all the parameter settings. Instead, it samples a fixed number of parameter settings based on the distributions you specify (e.g. you might specify that one parameter should be sampled uniformly while another is sampled following a Gaussian distribution). The number of parameter settings that are tried is given by `n_iter`. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. You should use continuous distributions for continuous parameters. Further details can be found at http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Conduct a randomised search on hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous example, we can print out the optimal parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the optimal n_neighbors detected by randomised search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also graphically represent the results of the randomised search using a scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the randomised search results using a scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, testing our independent XTest dataset using the optimised model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by randomised search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Module 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 19:  Random Forests\n",
    "\n",
    "The random forests model is an `ensemble method` since it aggregates a group of decision trees into an ensemble (http://scikit-learn.org/stable/modules/ensemble.html). Ensemble learning involves the combination of several models to solve a single prediction problem. It works by generating multiple classifiers/models which learn and make predictions independently. Those predictions are then combined into a single (mega) prediction that should be as good or better than the prediction made by any one classifer. Unlike single decision trees which are likely to suffer from high Variance or high Bias (depending on how they are tuned) Random Forests use averaging to find a natural balance between the two extremes. <br/> \n",
    "\n",
    "Let us start by building a simple Random Forest model which consists of 100 independently trained decision trees. For further details and examples on how to construct a Random Forest, see http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a Random Forest classifier with 100 decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 20: Visualising the RF accuracy\n",
    "\n",
    "We can also investigate how the overall test accuracy gets influenced with the increase of `n_estimators` (decision trees) in our model. In order to do so, we can use the provided `rfAvgAcc` function from `visplots`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise the average accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 21: Feature Importance \n",
    "\n",
    "Random forests allow you to compute a heuristic for determining how “important” a feature is in predicting a target. This heuristic measures the change in prediction accuracy if you take a given feature and permute (scramble) it across the datapoints in the training set. The more the accuracy drops when the feature is permuted, the more “important” we can conclude the feature is.\n",
    "\n",
    "We can use the `feature_importances_` attribute of the RF classifier to obtain the relative importance of each feature, which we can then visualise using a simple bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the importance of the features in a barplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Learning activity 22: Boundary visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the classification boundary created by the Random Forest using the `visplots.rfDecisionPlot` function. You can check the arguments passed in this function by using the `help` command. For easier visualisation, only the test samples have been included in the plot. And remember that the decision boundary has been built using the _training_ data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the arguments of the function\n",
    "\n",
    "# Visualise the boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Activity 23: Tuning Random Forests\n",
    "\n",
    "Random forests offer several parameters that can be tuned. In this case, parameters such as `n_estimators`, `max_features`, `max_depth` and `min_samples_leaf` can be some of the parameters to be optimised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the list of arguments to be optimised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of allowed parameter ranges for `n_estimators` and `max_depth` (or include more of the parameters you would like to tune) and conduct a grid search with cross validation using the `GridSearchCV` function as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Conduct a grid search with 10-fold cross-validation using the dictionary of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find and print the best parameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, testing our independent XTest dataset using the optimised model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the classifier using the optimal parameters detected by grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: We can also graphically represent the results of the grid search using a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise the grid search results using a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Learning Activity 24: Bonus - Parallelisation\n",
    "\n",
    "\n",
    "The scikit-learn implementation of Random Forests also features the parallel construction of the trees and the parallel computation of the predictions through the n_jobs parameter.\n",
    "If `n_jobs=k` then computations are partitioned into k jobs, and run on k cores of the machine.\n",
    "If `n_jobs=-1` then all cores available on the machine are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Build a RF classification model using parallelisation\n",
    "# 2. Try and tune its parameters using parallel processing\n",
    "# 3. Import the `timeit` module and use the `default_timer` function to calculate the speedup from sequential to parallel processing\n",
    "# 4. Can you plot the execution times with incremental number of processors?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
